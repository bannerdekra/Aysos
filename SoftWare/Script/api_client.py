import os
import requests
import json
import time

# æ£€æŸ¥ Google GenAI SDK æ˜¯å¦å¯ç”¨
GENAI_AVAILABLE = False
try:
    # å°è¯•ä¸¤ç§å¯¼å…¥æ–¹å¼
    try:
        from google import genai
        GENAI_AVAILABLE = True
        print("[OK] Google GenAI SDK (genai) å·²åŠ è½½")
    except ImportError:
        import google.generativeai as genai
        GENAI_AVAILABLE = True
        print("[OK] Google GenAI SDK (generativeai) å·²åŠ è½½")
except ImportError:
    print("[ERROR] Google GenAI SDK æœªå®‰è£…")
    GENAI_AVAILABLE = False

from api_config import (
    get_current_provider_config,
    get_current_provider_name,
    enable_proxy,
    disable_proxy,
    get_deepseek_api_key,
)

# å¯¼å…¥ Gemini ä¸Šä¸‹æ–‡ç®¡ç†å™¨
try:
    from gemini_context_manager import get_gemini_context_manager
    GEMINI_CONTEXT_AVAILABLE = True
    print("âœ… Gemini ä¸Šä¸‹æ–‡ç®¡ç†å™¨å·²å¯¼å…¥")
except ImportError:
    GEMINI_CONTEXT_AVAILABLE = False
    print("âš ï¸ Gemini ä¸Šä¸‹æ–‡ç®¡ç†å™¨æœªå¯¼å…¥")

# å¯¼å…¥å·¥å…·æ‰§è¡Œå™¨ï¼ˆFunction Callingï¼‰
try:
    from tool_executor import get_tool_executor, get_all_tool_schemas
    TOOL_EXECUTOR_AVAILABLE = True
    print("âœ… å·¥å…·æ‰§è¡Œå™¨å·²å¯¼å…¥")
except ImportError:
    TOOL_EXECUTOR_AVAILABLE = False
    print("âš ï¸ å·¥å…·æ‰§è¡Œå™¨æœªå¯¼å…¥")


# ä¿®æ”¹ä¸ºæ­£ç¡®çš„ç›¸å¯¹è·¯å¾„
SPINNER_GIF_URL = os.path.join('SoftWare', 'Image', 'loading', 'loading3.gif')

def get_ai_reply(messages, conversation_id=None, files=None, is_one_time_attachment=None, enable_tools=True):
    """
    Calls the configured AI API and returns the AI's reply.
    Supports DeepSeek, Gemini (with context and files), and other providers.
    Supports Function Calling for tool use.
    
    Args:
        messages (list): A list of dictionaries, where each dictionary represents a message
                         with 'role' and 'content' keys. This is the chat history.
        conversation_id (str, optional): å¯¹è¯IDï¼Œç”¨äº Gemini ä¸Šä¸‹æ–‡ç®¡ç†
        files (list, optional): æ–‡ä»¶ä¿¡æ¯ï¼Œæ”¯æŒä¸¤ç§æ ¼å¼ï¼š
                               1. å­—å…¸åˆ—è¡¨ [{'path': str, 'mode': str, 'file_id': str}]
                               2. ç®€å•è·¯å¾„åˆ—è¡¨ [str]ï¼ˆé…åˆ is_one_time_attachment å‚æ•°ï¼‰
        is_one_time_attachment (bool, optional): å¦‚æœä¸º Trueï¼Œä½¿ç”¨ Base64 å†…åµŒï¼ˆä»…æœ¬æ¬¡ä½¿ç”¨ï¼‰ï¼›
                                                å¦‚æœä¸º Falseï¼Œä½¿ç”¨ File API ä¸Šä¼ ï¼ˆä¸Šä¸‹æ–‡æŒä¹…åŒ–ï¼‰ã€‚
                                                å½“ files ä¸ºå­—å…¸åˆ—è¡¨æ—¶ï¼Œæ­¤å‚æ•°è¢«å¿½ç•¥ã€‚
        enable_tools (bool, optional): æ˜¯å¦å¯ç”¨ Function Calling å·¥å…·è°ƒç”¨ï¼ˆé»˜è®¤Trueï¼‰
    """
    
    provider_name = get_current_provider_name()
    _apply_proxy_policy(provider_name)
    provider_config = get_current_provider_config()
    
    api_key = (provider_config.get('api_key') or '').strip()
    api_url = (provider_config.get('api_url') or '').strip()
    model_name = (provider_config.get('model') or '').strip()

    # å¯¹äº Geminiï¼Œç›´æ¥ä½¿ç”¨ç³»ç»Ÿç¯å¢ƒå˜é‡ GEMINI_API_KEY
    if provider_name == 'gemini':
        # Gemini ä½¿ç”¨ç³»ç»Ÿç¯å¢ƒå˜é‡ï¼Œä¸éœ€è¦åœ¨è¿™é‡Œæ£€æŸ¥ API key
        pass
    else:
        if provider_name == 'deepseek':
            env_key = get_deepseek_api_key()
            if env_key:
                api_key = env_key.strip()
            else:
                print("[WARNING] DeepSeek ç¯å¢ƒå˜é‡æœªè®¾ç½®: DeepSeek-APIKEY")

        # å¯¹äºå…¶ä»–æä¾›å•†ï¼Œéœ€è¦ API key å’Œ URL
        if not api_key or not api_url:
            return f"Error: API key or URL is not configured for {provider_name}. Please set it in Settings > API."

    try:
        if provider_name == 'deepseek':
            return _call_deepseek_api(messages, api_key, api_url, model_name, enable_tools=enable_tools)
        elif provider_name == 'gemini':
            return _call_gemini_api_with_context(messages, api_key, model_name, conversation_id, files, is_one_time_attachment, enable_tools=enable_tools)
        else:
            # Default to OpenAI-compatible format
            return _call_openai_compatible_api(messages, api_key, api_url, model_name, enable_tools=enable_tools)
            
    except requests.exceptions.Timeout:
        return f"Error: {provider_name} API request timed out after 120 seconds."
    except Exception as e:
        return f"Error calling {provider_name} API: {str(e)}"


def _call_deepseek_api(messages, api_key, api_url, model_name, enable_tools=True, max_iterations=5):
    """
    Call DeepSeek API (OpenAI-compatible format) with Function Calling support.
    æ”¯æŒå¤šè½®å·¥å…·è°ƒç”¨è¿­ä»£ï¼Œç›´åˆ° AI ç»™å‡ºæœ€ç»ˆç­”æ¡ˆã€‚
    
    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨
        api_key: APIå¯†é’¥
        api_url: APIåœ°å€
        model_name: æ¨¡å‹åç§°
        enable_tools: æ˜¯å¦å¯ç”¨å·¥å…·è°ƒç”¨
        max_iterations: æœ€å¤§å·¥å…·è°ƒç”¨è¿­ä»£æ¬¡æ•°ï¼ˆé˜²æ­¢æ— é™å¾ªç¯ï¼‰
    
    Returns:
        str: AIå›å¤æ–‡æœ¬
    """
    try:
        # å‡†å¤‡åŸºç¡€ payload
        payload = {
            "messages": messages.copy()  # ä½¿ç”¨å‰¯æœ¬é¿å…ä¿®æ”¹åŸå§‹æ¶ˆæ¯
        }
        if model_name:
            payload["model"] = model_name

        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        # å¦‚æœå¯ç”¨å·¥å…·ä¸”å·¥å…·æ‰§è¡Œå™¨å¯ç”¨ï¼Œæ·»åŠ å·¥å…·å®šä¹‰å’Œå¼•å¯¼æç¤º
        tool_schemas = None
        if enable_tools and TOOL_EXECUTOR_AVAILABLE:
            try:
                tool_schemas = get_all_tool_schemas()
                if tool_schemas:
                    payload["tools"] = tool_schemas
                    print(f"[DeepSeek] ğŸ“¦ å·²æ·»åŠ  {len(tool_schemas)} ä¸ªå·¥å…·")
                    
                    # ğŸ”§ åœ¨ç³»ç»Ÿæ¶ˆæ¯ä¸­æ·»åŠ å·¥å…·ä½¿ç”¨å¼•å¯¼
                    if not any(msg.get('role') == 'system' for msg in payload["messages"]):
                        payload["messages"].insert(0, {
                            "role": "system",
                            "content": (
                                "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œæ‹¥æœ‰æœç´¢å·¥å…·ã€‚\n"
                                "é‡è¦è§„åˆ™ï¼š\n"
                                "1. å½“ç”¨æˆ·è¯¢é—®ä½ ä¸äº†è§£çš„ä¿¡æ¯ï¼ˆå¦‚å®æ—¶æ–°é—»ã€å¤©æ°”ã€è‚¡ç¥¨ç­‰ï¼‰æ—¶ï¼Œå¿…é¡»ä¸»åŠ¨è°ƒç”¨æœç´¢å·¥å…·\n"
                                "2. å¦‚æœç¬¬ä¸€æ¬¡æœç´¢ç»“æœä¸å‡†ç¡®ï¼Œç»§ç»­è°ƒç”¨å·¥å…·ç›´åˆ°è·å¾—æ­£ç¡®ä¿¡æ¯\n"
                                "3. åªæœ‰åœ¨ç¡®è®¤è·å¾—å‡†ç¡®ç­”æ¡ˆåæ‰å›å¤ç”¨æˆ·\n"
                                "4. ä¼˜å…ˆä½¿ç”¨å·¥å…·è€ŒéçŒœæµ‹æˆ–ä½¿ç”¨è¿‡æ—¶ä¿¡æ¯"
                            )
                        })
            except Exception as e:
                print(f"[DeepSeek] âš ï¸ å·¥å…·Schemaè·å–å¤±è´¥: {e}")
        
        # ğŸ”§ å¤šè½®å·¥å…·è°ƒç”¨å¾ªç¯
        iteration = 0
        conversation_messages = payload["messages"].copy()
        
        while iteration < max_iterations:
            iteration += 1
            print(f"[DeepSeek] ğŸ“¤ ç¬¬ {iteration} è½® API è°ƒç”¨...")
            
            # æ›´æ–° payload æ¶ˆæ¯
            payload["messages"] = conversation_messages
            
            # API è°ƒç”¨
            response = requests.post(api_url, json=payload, headers=headers, timeout=120)
            response.raise_for_status()
            data = response.json()
            
            message = data['choices'][0]['message']
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
            if 'tool_calls' in message and message['tool_calls'] and TOOL_EXECUTOR_AVAILABLE:
                print(f"[DeepSeek] ğŸ”§ ç¬¬ {iteration} è½®æ£€æµ‹åˆ° {len(message['tool_calls'])} ä¸ªå·¥å…·è°ƒç”¨")
                
                # æ‰§è¡Œå·¥å…·è°ƒç”¨
                tool_executor = get_tool_executor()
                tool_results = tool_executor.execute_tool_calls(message['tool_calls'])
                
                # æ·»åŠ åŠ©æ‰‹çš„å·¥å…·è°ƒç”¨æ¶ˆæ¯
                conversation_messages.append({
                    "role": "assistant",
                    "content": message.get('content', ''),
                    "tool_calls": message['tool_calls']
                })
                
                # æ·»åŠ å·¥å…·æ‰§è¡Œç»“æœ
                for tool_result in tool_results:
                    if tool_result['success']:
                        conversation_messages.append({
                            "role": "tool",
                            "tool_call_id": tool_result.get('tool_call_id', ''),
                            "name": tool_result['tool_name'],
                            "content": tool_result['result_json']
                        })
                    else:
                        conversation_messages.append({
                            "role": "tool",
                            "tool_call_id": tool_result.get('tool_call_id', ''),
                            "name": tool_result.get('tool_name', 'unknown'),
                            "content": json.dumps({"error": tool_result['error']}, ensure_ascii=False)
                        })
                
                # ç»§ç»­ä¸‹ä¸€è½®è¿­ä»£ï¼ˆAI ä¼šåŸºäºå·¥å…·ç»“æœå†³å®šæ˜¯å¦ç»§ç»­è°ƒç”¨å·¥å…·ï¼‰
                print(f"[DeepSeek] ğŸ”„ å·¥å…·æ‰§è¡Œå®Œæˆï¼Œç­‰å¾… AI ä¸‹ä¸€æ­¥å†³ç­–...")
                continue
            
            # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œè¿”å›æœ€ç»ˆç»“æœ
            final_content = message.get('content', '')
            if iteration > 1:
                print(f"[DeepSeek] âœ… ç»è¿‡ {iteration} è½®è¿­ä»£ï¼Œè·å¾—æœ€ç»ˆç­”æ¡ˆ")
            return final_content
        
        # è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°
        print(f"[DeepSeek] âš ï¸ è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° ({max_iterations})ï¼Œè¿”å›å½“å‰ç»“æœ")
        return message.get('content', '') if message else "æŠ±æ­‰ï¼Œå¤„ç†è¶…æ—¶ï¼Œè¯·é‡è¯•ã€‚"
        
    except requests.exceptions.Timeout:
        print(f"[DeepSeek] âŒ è¯·æ±‚è¶…æ—¶")
        return "Error: API è¯·æ±‚è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•ã€‚"
    except requests.exceptions.RequestException as e:
        print(f"[DeepSeek] âŒ ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
        return f"Error: ç½‘ç»œè¯·æ±‚å¤±è´¥ - {str(e)}"
    except KeyError as e:
        print(f"[DeepSeek] âŒ API å“åº”æ ¼å¼é”™è¯¯: {e}")
        return f"Error: API å“åº”æ ¼å¼å¼‚å¸¸ - {str(e)}"
    except Exception as e:
        print(f"[DeepSeek] âŒ æœªçŸ¥é”™è¯¯: {e}")
        return f"Error: å¤„ç†å¤±è´¥ - {str(e)}"


def _apply_proxy_policy(provider_name: str) -> None:
    """Enable or disable proxy based on provider selection."""
    if provider_name == 'gemini':
        enable_proxy()
    else:
        disable_proxy()


def _convert_openai_tools_to_gemini(openai_tools: list):
    """
    å°† OpenAI æ ¼å¼çš„å·¥å…· Schema è½¬æ¢ä¸º Gemini æ ¼å¼
    
    OpenAI æ ¼å¼ç¤ºä¾‹ï¼š
    {
        "type": "function",
        "function": {
            "name": "baidu_search",
            "description": "...",
            "parameters": {...}
        }
    }
    
    Gemini æ ¼å¼ï¼šä½¿ç”¨ google.genai.types.Tool
    
    Args:
        openai_tools: OpenAI æ ¼å¼çš„å·¥å…·åˆ—è¡¨
    
    Returns:
        types.Tool å¯¹è±¡æˆ– None
    """
    if not openai_tools or not GENAI_AVAILABLE:
        return None
    
    try:
        from google.genai import types
        
        function_declarations = []
        
        for tool in openai_tools:
            if tool.get("type") == "function":
                func = tool.get("function", {})
                function_declarations.append({
                    "name": func.get("name"),
                    "description": func.get("description"),
                    "parameters": func.get("parameters", {})
                })
        
        if function_declarations:
            # ä½¿ç”¨ types.Tool åŒ…è£… function_declarations
            gemini_tool = types.Tool(function_declarations=function_declarations)
            print(f"[Gemini] âœ… æˆåŠŸè½¬æ¢ {len(function_declarations)} ä¸ªå·¥å…·")
            return gemini_tool
        
        return None
        
    except Exception as e:
        print(f"[Gemini] âŒ å·¥å…·è½¬æ¢å¤±è´¥: {e}")
        return None


def _extract_function_calls_from_response(response):
    """
    ä» Gemini Response å¯¹è±¡ä¸­æå– function_call ä¿¡æ¯
    
    æ ¹æ® Gemini å®˜æ–¹æ–‡æ¡£ï¼Œfunction_call ä½äºï¼š
    response.candidates[0].content.parts[i].function_call
    
    Args:
        response: Gemini API response å¯¹è±¡
    
    Returns:
        list: function_call åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º {"name": ..., "args": {...}}
    """
    function_calls = []
    
    try:
        if not response or not hasattr(response, 'candidates'):
            return []
        
        for candidate in response.candidates:
            if not hasattr(candidate, 'content') or not hasattr(candidate.content, 'parts'):
                continue
            
            for part in candidate.content.parts:
                if hasattr(part, 'function_call'):
                    fc = part.function_call
                    function_calls.append({
                        'name': fc.name,
                        'args': dict(fc.args) if hasattr(fc, 'args') else {}
                    })
        
    except Exception as e:
        print(f"[Gemini] âš ï¸ æå– function_call å¤±è´¥: {e}")
    
    return function_calls


def _call_gemini_api_with_context(messages, api_key, model_name, conversation_id=None, files=None, is_one_time_attachment=None, enable_tools=True):
    """
    Call Google Gemini API with context management (Chat Session).
    æ”¯æŒä¸Šä¸‹æ–‡è®°å¿†å’Œæ–‡ä»¶ä¸Šä¼ çš„ Gemini API è°ƒç”¨ã€‚
    æ”¯æŒ Function Calling å·¥å…·è°ƒç”¨ã€‚
    
    æ ¸å¿ƒé€»è¾‘ï¼š
    1. æ£€æŸ¥å¹¶æ¢å¤ Chat Session å†å²ï¼ˆå¦‚æœæœªåˆå§‹åŒ–ï¼‰
    2. Chat Session å†…éƒ¨è‡ªåŠ¨ç»´æŠ¤å†å²è®°å½•
    3. åªéœ€å‘é€æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
    4. æ”¯æŒä¸¤ç§æ–‡ä»¶æ¨¡å¼ï¼šä¸´æ—¶ï¼ˆå†…åµŒï¼‰å’ŒæŒä¹…ï¼ˆFile APIï¼‰
    5. å†å²åŒæ­¥ç”± Chat Session ç®¡ç†ï¼Œæ— éœ€æ‰‹åŠ¨å¤„ç†
    6. æ”¯æŒ Function Callingï¼ˆå¦‚æœå¯ç”¨ï¼‰
    
    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨ï¼ˆOpenAIæ ¼å¼ï¼Œç”¨äºå†å²æ¢å¤å’Œæå–æœ€æ–°æ¶ˆæ¯ï¼‰
        api_key: API å¯†é’¥ï¼ˆä»ç¯å¢ƒå˜é‡è·å–ï¼‰
        model_name: æ¨¡å‹åç§°
        conversation_id: å¯¹è¯IDï¼Œç”¨äºç®¡ç†ä¸Šä¸‹æ–‡
        files: æ–‡ä»¶ä¿¡æ¯ï¼ˆå­—å…¸åˆ—è¡¨æˆ–è·¯å¾„åˆ—è¡¨ï¼‰
        is_one_time_attachment: æ–‡ä»¶æ¨¡å¼æ ‡è®°ï¼ˆä»…åœ¨ files ä¸ºè·¯å¾„åˆ—è¡¨æ—¶ä½¿ç”¨ï¼‰
        enable_tools: æ˜¯å¦å¯ç”¨å·¥å…·è°ƒç”¨
    """
    if not GENAI_AVAILABLE:
        return "Error: Google GenAI SDK is not installed. Please run 'pip install google-generativeai'"
    
    # æ£€æŸ¥æ˜¯å¦å¯ç”¨ä¸Šä¸‹æ–‡ç®¡ç†
    if not GEMINI_CONTEXT_AVAILABLE or not conversation_id:
        # é™çº§åˆ°æ— ä¸Šä¸‹æ–‡æ¨¡å¼
        print("âš ï¸ ä½¿ç”¨æ— ä¸Šä¸‹æ–‡æ¨¡å¼ï¼ˆæœªæä¾› conversation_id æˆ–ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¸å¯ç”¨ï¼‰")
        return _call_gemini_api_with_sdk(messages, api_key, model_name)
    
    try:
        # è·å–ä¸Šä¸‹æ–‡ç®¡ç†å™¨
        context_manager = get_gemini_context_manager()
        if not context_manager:
            print("âš ï¸ ä¸Šä¸‹æ–‡ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥ï¼Œé™çº§åˆ°æ— ä¸Šä¸‹æ–‡æ¨¡å¼")
            return _call_gemini_api_with_sdk(messages, api_key, model_name)
        
        # æ£€æŸ¥ç³»ç»Ÿç¯å¢ƒå˜é‡ GEMINI_API_KEY
        env_api_key = os.getenv('GEMINI_API_KEY')
        if not env_api_key:
            return "Error: GEMINI_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®ã€‚è¯·åœ¨ç³»ç»Ÿä¸­è®¾ç½® GEMINI_API_KEY ç¯å¢ƒå˜é‡ã€‚"
        
        # ç¡®ä¿æ¨¡å‹åç§°æœ‰æ•ˆ
        model_to_use = model_name if model_name else 'gemini-2.5-flash'
        print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {model_to_use} (å¯ç”¨ä¸Šä¸‹æ–‡)")
        
        # ã€æ ¸å¿ƒä¿®æ­£ã€‘åœ¨å‘é€æ¶ˆæ¯å‰ï¼Œæ£€æŸ¥å¹¶æ¢å¤å†å²
        # åªæœ‰å½“ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¸­ä¸å­˜åœ¨æ­¤ä¼šè¯æ—¶æ‰éœ€è¦æ¢å¤
        if not context_manager.get_chat_session(conversation_id):
            print(f"ğŸ” ä¼šè¯ {conversation_id} ä¸å­˜åœ¨ï¼Œå°è¯•ä»å†å²æ¢å¤...")
            # å¦‚æœ messages åŒ…å«å¤šæ¡å†å²è®°å½•ï¼ˆä¸åªæ˜¯å½“å‰æ¶ˆæ¯ï¼‰ï¼Œåˆ™éœ€è¦æ¢å¤
            if len(messages) > 1:
                # æ’é™¤æœ€åä¸€æ¡æ¶ˆæ¯ï¼ˆè¿™æ˜¯å½“å‰è¦å‘é€çš„ï¼‰
                history_to_restore = messages[:-1]
                if history_to_restore:
                    print(f"ğŸ“š æ¢å¤ {len(history_to_restore)} æ¡å†å²è®°å½•...")
                    try:
                        context_manager.restore_chat_history(
                            conversation_id=conversation_id,
                            messages=history_to_restore,
                            model=model_to_use
                        )
                    except Exception as e:
                        print(f"âš ï¸ å†å²æ¢å¤å¤±è´¥: {str(e)}")
                        # ç»§ç»­æ‰§è¡Œï¼Œåˆ›å»ºæ–°çš„ Chat Session
            else:
                print(f"ğŸ“ è¿™æ˜¯æ–°å¯¹è¯çš„ç¬¬ä¸€æ¡æ¶ˆæ¯")
        else:
            print(f"âœ… ä½¿ç”¨ç°æœ‰çš„ Chat Session")
        
        # ã€æ ¸å¿ƒã€‘æå–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ï¼ˆè¿™æ˜¯æœ¬æ¬¡è¯·æ±‚çš„å”¯ä¸€æ–°å¢å†…å®¹ï¼‰
        # Chat Session ä¼šè‡ªåŠ¨å°†å…¶ä¸å†…éƒ¨å†å²åˆå¹¶
        last_user_message = None
        for msg in reversed(messages):
            if msg['role'] == 'user':
                last_user_message = msg['content']
                break
        
        if not last_user_message:
            return "Error: No user message found."
        
        # è·å–å¹¶è½¬æ¢å·¥å…· Schemaï¼ˆå¦‚æœå¯ç”¨ï¼‰
        gemini_tools = None
        if enable_tools and TOOL_EXECUTOR_AVAILABLE:
            try:
                openai_tools = get_all_tool_schemas()
                if openai_tools:
                    gemini_tools = _convert_openai_tools_to_gemini(openai_tools)
                    print(f"[Gemini] ğŸ”§ å·²å‡†å¤‡ {len(openai_tools)} ä¸ªå·¥å…·ï¼ˆGeminiæ ¼å¼ï¼‰")
            except Exception as e:
                print(f"[Gemini] âš ï¸ å·¥å…·å‡†å¤‡å¤±è´¥: {e}")
        
        # åˆ¤æ–­æ˜¯å¦æœ‰æ–‡ä»¶
        if files and len(files) > 0:
            # ã€å¸¦æ–‡ä»¶ã€‘å‘é€æ¶ˆæ¯
            # æ–‡ä»¶ç°åœ¨æ˜¯å­—å…¸åˆ—è¡¨ï¼ŒåŒ…å« path, mode, file_id
            print(f"ğŸ“¤ å‘é€æ¶ˆæ¯åˆ° Chat Sessionï¼ˆå« {len(files)} ä¸ªæ–‡ä»¶ï¼‰")
            
            # åˆ†ç¦»ä¸´æ—¶æ–‡ä»¶å’ŒæŒä¹…æ–‡ä»¶
            temporary_files = [f['path'] for f in files if f.get('mode') == 'temporary']
            persistent_file_ids = [f['file_id'] for f in files if f.get('mode') == 'persistent' and f.get('file_id')]
            
            print(f"ï¿½ ä¸´æ—¶æ–‡ä»¶: {len(temporary_files)} ä¸ª")
            print(f"ğŸ”— æŒä¹…æ–‡ä»¶: {len(persistent_file_ids)} ä¸ª")
            
            for f in files:
                mode_icon = 'ğŸ“„' if f.get('mode') == 'temporary' else 'ğŸ”—'
                print(f"  {mode_icon} {f['path']} (mode={f.get('mode')})")
            
            response = context_manager.send_message_with_files(
                conversation_id=conversation_id,
                message=last_user_message,
                file_paths=temporary_files,  # ä¸´æ—¶æ–‡ä»¶è·¯å¾„
                persistent_file_ids=persistent_file_ids,  # æŒä¹…æ–‡ä»¶ID
                model=model_to_use,
                tools=gemini_tools  # ä¼ é€’å·¥å…·
            )
        else:
            # ã€çº¯æ–‡æœ¬ã€‘å‘é€æ¶ˆæ¯
            # GeminiContextManager.send_message() å†…éƒ¨ä¼šï¼š
            # 1. è·å–æˆ–åˆ›å»ºå¯¹åº”çš„ Chat Session
            # 2. å°†æ¶ˆæ¯æ·»åŠ åˆ°å†…éƒ¨å†å²
            # 3. å‘é€ [å†…éƒ¨å†å² + æœ¬æ¬¡æ¶ˆæ¯] ç»™æ¨¡å‹
            # 4. å°†æ¨¡å‹å›å¤ä¹Ÿæ·»åŠ åˆ°å†…éƒ¨å†å²
            print(f"ğŸ“¤ å‘é€æ¶ˆæ¯åˆ° Chat Session: {last_user_message[:50]}...")
            response = context_manager.send_text_message(
                conversation_id=conversation_id,
                message=last_user_message,
                model=model_to_use,
                tools=gemini_tools  # ä¼ é€’å·¥å…·
            )
        
        # æ£€æŸ¥ response ç±»å‹
        if isinstance(response, str):
            # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œè¯´æ˜æ˜¯é”™è¯¯æ¶ˆæ¯
            if response.startswith("Error:"):
                raise Exception(response)
            return response
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ function_callï¼ˆæ ¹æ® Gemini å®˜æ–¹æ–‡æ¡£ï¼‰
        # response.candidates[0].content.parts ä¸­å¯èƒ½åŒ…å« function_call
        chat = context_manager.get_chat_session(conversation_id)
        function_calls = _extract_function_calls_from_response(response)
        
        if function_calls and TOOL_EXECUTOR_AVAILABLE:
            print(f"[Gemini] ğŸ”§ æ£€æµ‹åˆ° {len(function_calls)} ä¸ªå·¥å…·è°ƒç”¨")
            from tool_executor import execute_tool
            from google.genai import types
            
            # æ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨å¹¶æ„é€  function_response
            function_response_parts = []
            for fc in function_calls:
                tool_name = fc.get('name')
                tool_args = fc.get('args', {})
                print(f"[Gemini] ğŸ“ è°ƒç”¨å·¥å…·: {tool_name}ï¼Œå‚æ•°: {tool_args}")
                result = execute_tool(tool_name, tool_args)
                print(f"[Gemini] âœ… å·¥å…· {tool_name} è¿”å›: {str(result)[:100]}...")
                
                # æ ¹æ® Gemini å®˜æ–¹æ–‡æ¡£ï¼Œä½¿ç”¨ Part åŒ…è£… FunctionResponse
                function_response_parts.append(
                    types.Part.from_function_response(
                        name=tool_name,
                        response={'result': result}
                    )
                )
            
            # å‘é€ function_response åˆ° Chat Session
            # æ ¹æ® Gemini SDKï¼Œsend_message æ¥å—å¤šä¸ª Part ä½œä¸ºç‹¬ç«‹å‚æ•°ï¼Œè€Œä¸æ˜¯åˆ—è¡¨
            print(f"[Gemini] ğŸ“¤ å‘é€å·¥å…·ç»“æœåˆ°æ¨¡å‹...")
            final_response = chat.send_message(*function_response_parts)
            
            # æå–æœ€ç»ˆæ–‡æœ¬å›å¤
            response_text = context_manager._extract_text_from_response(final_response)
            print(f"âœ… æ”¶åˆ°æœ€ç»ˆå›å¤: {response_text[:50] if response_text else 'Empty'}...")
            return response_text
        else:
            # æ²¡æœ‰ function_callï¼Œç›´æ¥æå–æ–‡æœ¬
            response_text = context_manager._extract_text_from_response(response)
            print(f"âœ… æ”¶åˆ°å›å¤: {response_text[:50] if response_text else 'Empty'}...")
            return response_text
        
    except Exception as e:
        print(f"âŒ Gemini ä¸Šä¸‹æ–‡ API é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        # é™çº§åˆ°æ— ä¸Šä¸‹æ–‡æ¨¡å¼
        print("âš ï¸ é™çº§åˆ°æ— ä¸Šä¸‹æ–‡æ¨¡å¼")
        return _call_gemini_api_with_sdk(messages, api_key, model_name)


def _call_gemini_api_with_sdk(messages, api_key, model_name):
    """Call Google Gemini API using the official Google GenAI SDK with Client() (æ— ä¸Šä¸‹æ–‡æ¨¡å¼)."""
    if not GENAI_AVAILABLE:
        return "Error: Google GenAI SDK is not installed. Please run 'pip install google-generativeai'"
    
    try:
        # æ£€æŸ¥ç³»ç»Ÿç¯å¢ƒå˜é‡ GEMINI_API_KEY
        env_api_key = os.getenv('GEMINI_API_KEY')
        if not env_api_key:
            return "Error: GEMINI_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®ã€‚è¯·åœ¨ç³»ç»Ÿä¸­è®¾ç½® GEMINI_API_KEY ç¯å¢ƒå˜é‡ã€‚"
        
        print(f"âœ… æ‰¾åˆ°ç³»ç»Ÿç¯å¢ƒå˜é‡ GEMINI_API_KEY")
        
        # ä½¿ç”¨æ‚¨æä¾›çš„å®ä¾‹æ ¼å¼ï¼šfrom google import genai
        from google import genai
        
        # The client gets the API key from the environment variable `GEMINI_API_KEY`.
        client = genai.Client()

        # ç¡®ä¿ä½¿ç”¨ gemini-2.5-flash æ¨¡å‹ï¼ˆæ ¹æ®æ‚¨çš„å¿«é€Ÿå…¥é—¨æŒ‡ä»¤ï¼‰
        # Use the provided model name, or default to 'gemini-2.5-flash'
        model_to_use = model_name if model_name else 'gemini-2.5-flash'
        print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {model_to_use}")

        # ç®€åŒ–æ¶ˆæ¯å¤„ç† - åªå–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
        last_user_message = None
        for msg in reversed(messages):
            if msg['role'] == 'user':
                last_user_message = msg['content']
                break
        
        if not last_user_message:
            return "Error: No user message found."

        print(f"ğŸ“¤ å‘é€æ¶ˆæ¯: {last_user_message[:50]}...")
        
        # ä½¿ç”¨æ‚¨æä¾›çš„æ ¼å¼è°ƒç”¨ API
        response = client.models.generate_content(
            model=model_to_use, 
            contents=last_user_message
        )
        
        if response and response.text:
            print("âœ… æ”¶åˆ°å›å¤")
            return response.text
        else:
            return "Error: Empty response from Gemini."
            
    except Exception as e:
        print(f"âŒ Gemini API é”™è¯¯: {str(e)}")
        return f"Error calling Gemini API: {str(e)}"


def _call_gemini_api(messages, api_key, api_url, model_name, enable_tools=True):
    """
    Call Google Gemini API (fallback REST API method).
    
    Args:
        messages: List of message dicts
        api_key: API key for Gemini
        api_url: Base URL for Gemini API
        model_name: Model identifier
        enable_tools: Whether to enable function calling tools (default: True)
    
    Returns:
        str: The AI response text
    """
    # Gemini API uses a different format
    # Convert OpenAI format to Gemini format
    
    gemini_messages = []
    for msg in messages:
        if msg['role'] == 'system':
            # Gemini doesn't have system role, prepend to first user message
            continue
        elif msg['role'] == 'user':
            gemini_messages.append({
                "role": "user",
                "parts": [{"text": msg['content']}]
            })
        elif msg['role'] == 'assistant':
            gemini_messages.append({
                "role": "model", 
                "parts": [{"text": msg['content']}]
            })
    
    # Add system message content to first user message if exists
    system_content = ""
    for msg in messages:
        if msg['role'] == 'system':
            system_content = msg['content'] + "\n\n"
            break
    
    if gemini_messages and system_content:
        gemini_messages[0]['parts'][0]['text'] = system_content + gemini_messages[0]['parts'][0]['text']
    
    payload = {
        "contents": gemini_messages,
        "generationConfig": {
            "temperature": 0.7,
            "maxOutputTokens": 2048,
        }
    }
    
    # Add tools if enabled and available
    if enable_tools and TOOL_EXECUTOR_AVAILABLE:
        tools_schemas = get_all_tool_schemas()
        # Convert OpenAI format to Gemini format
        gemini_tools = []
        for tool_schema in tools_schemas:
            if tool_schema.get('type') == 'function':
                func = tool_schema['function']
                gemini_tools.append({
                    "function_declarations": [{
                        "name": func['name'],
                        "description": func['description'],
                        "parameters": func['parameters']
                    }]
                })
        if gemini_tools:
            payload["tools"] = gemini_tools
    
    # Build the URL with model and API key
    full_url = f"{api_url}/{model_name}:generateContent?key={api_key}"
    
    headers = {
        "Content-Type": "application/json"
    }
    
    # First API call
    response = requests.post(full_url, json=payload, headers=headers, timeout=120)
    response.raise_for_status()
    data = response.json()
    
    if 'candidates' in data and len(data['candidates']) > 0:
        candidate = data['candidates'][0]
        if 'content' in candidate:
            content = candidate['content']
            
            # Check if there are function calls
            if 'parts' in content:
                parts = content['parts']
                # Check if any part contains function call
                has_function_call = any('functionCall' in part for part in parts)
                
                if has_function_call and enable_tools and TOOL_EXECUTOR_AVAILABLE:
                    # Extract function calls and execute them
                    tool_executor = get_tool_executor()
                    function_responses = []
                    
                    for part in parts:
                        if 'functionCall' in part:
                            func_call = part['functionCall']
                            tool_name = func_call.get('name', '')
                            tool_args = func_call.get('args', {})
                            
                            # Execute the tool
                            result = tool_executor.execute_tool(tool_name, tool_args)
                            
                            if result['success']:
                                function_responses.append({
                                    "functionResponse": {
                                        "name": tool_name,
                                        "response": {
                                            "content": result['result_json']
                                        }
                                    }
                                })
                            else:
                                function_responses.append({
                                    "functionResponse": {
                                        "name": tool_name,
                                        "response": {
                                            "error": result.get('error', 'Unknown error')
                                        }
                                    }
                                })
                    
                    if function_responses:
                        # Add assistant's function call and function responses to conversation
                        gemini_messages.append({
                            "role": "model",
                            "parts": parts
                        })
                        
                        gemini_messages.append({
                            "role": "function",
                            "parts": function_responses
                        })
                        
                        # Second API call with function results
                        payload["contents"] = gemini_messages
                        # Remove tools from second call as per Gemini best practices
                        if "tools" in payload:
                            del payload["tools"]
                        
                        response = requests.post(full_url, json=payload, headers=headers, timeout=120)
                        response.raise_for_status()
                        data = response.json()
                        
                        if 'candidates' in data and len(data['candidates']) > 0:
                            candidate = data['candidates'][0]
                            if 'content' in candidate and 'parts' in candidate['content']:
                                # Get final text response
                                final_parts = candidate['content']['parts']
                                text_parts = [p['text'] for p in final_parts if 'text' in p]
                                if text_parts:
                                    return ' '.join(text_parts)
                        
                        return "Error: No valid response after tool execution."
                
                # No function call, return text response
                text_parts = [p['text'] for p in parts if 'text' in p]
                if text_parts:
                    return ' '.join(text_parts)
    
    return "Error: Invalid response format from Gemini API."


def _call_openai_compatible_api(messages, api_key, api_url, model_name, enable_tools=True):
    """
    Call OpenAI-compatible API (fallback for other providers).
    
    Args:
        messages: List of message dicts
        api_key: API key for authentication
        api_url: Base URL for the API endpoint
        model_name: Model identifier
        enable_tools: Whether to enable function calling tools (default: True)
    
    Returns:
        str: The AI response text
    """
    payload = {
        "messages": messages
    }
    if model_name:
        payload["model"] = model_name
    
    # Add tools if enabled and available
    if enable_tools and TOOL_EXECUTOR_AVAILABLE:
        tools_schemas = get_all_tool_schemas()
        if tools_schemas:
            payload["tools"] = tools_schemas
            payload["tool_choice"] = "auto"

    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    # First API call
    response = requests.post(api_url, json=payload, headers=headers, timeout=120)
    response.raise_for_status()
    data = response.json()
    
    message = data['choices'][0]['message']
    
    # Check if there are tool calls
    if 'tool_calls' in message and message['tool_calls'] and enable_tools and TOOL_EXECUTOR_AVAILABLE:
        # Execute all tool calls
        tool_executor = get_tool_executor()
        tool_results = tool_executor.execute_tool_calls(message['tool_calls'])
        
        # Build messages with tool results
        messages_with_tools = messages.copy()
        
        # Add assistant message with tool calls
        messages_with_tools.append({
            "role": "assistant",
            "content": message.get('content') or '',
            "tool_calls": message['tool_calls']
        })
        
        # Add tool results
        for result in tool_results:
            messages_with_tools.append({
                "role": "tool",
                "content": result['result_json'],
                "tool_call_id": result['tool_call_id']
            })
        
        # Second API call with tool results
        payload["messages"] = messages_with_tools
        # Remove tool_choice for second call
        if "tool_choice" in payload:
            del payload["tool_choice"]
        
        response = requests.post(api_url, json=payload, headers=headers, timeout=120)
        response.raise_for_status()
        data = response.json()
        
        return data['choices'][0]['message']['content']
    
    # No tool calls, return content directly
    return message.get('content', '')


# Keep the old function name for backward compatibility
def get_deepseek_reply(messages):
    """Backward compatibility function."""
    return get_ai_reply(messages)

def get_topic_from_reply(ai_response):
    """
    é€šè¿‡ä¸€ä¸ªç‰¹æ®Šçš„APIè¯·æ±‚ï¼Œä»AIå›å¤å†…å®¹ä¸­æå–ä¸€ä¸ªç®€çŸ­çš„å¯¹è¯ä¸»é¢˜ã€‚
    ç‰¹åˆ«é€‚ç”¨äºåŒ…å«å›¾ç‰‡åˆ†æã€æ–‡æ¡£åˆ†æç­‰AIç”Ÿæˆå†…å®¹çš„æƒ…å†µã€‚
    """
    messages = [
        {"role": "system", "content": "ä½ æ˜¯æ ‡é¢˜ç”ŸæˆåŠ©æ‰‹ã€‚æ ¹æ®AIå›å¤å†…å®¹ç”Ÿæˆ3ä¸ªæ ‡é¢˜ï¼Œæ¯ä¸ª5å­—ä»¥å†…ï¼Œç”¨é€—å·åˆ†éš”ã€‚ç¤ºä¾‹æ ¼å¼ï¼šè§†é¢‘åˆ†æ,å†…å®¹è§£è¯»,å¤šåª’ä½“è®¨è®ºã€‚åªè¿”å›æ ‡é¢˜ï¼Œä¸è¦ç†ç”±ã€ä¸è¦æ¢è¡Œã€ä¸è¦åºå·ã€ä¸è¦å…¶ä»–ä»»ä½•å†…å®¹ã€‚"},
        {"role": "user", "content": f"AIå›å¤å†…å®¹ï¼š{ai_response[:200]}"}  # åªå–å‰200å­—é¿å…è¿‡é•¿
    ]
    
    try:
        reply = get_ai_reply(messages)
        # æ¸…ç†è¿”å›å†…å®¹
        clean_reply = reply.strip().replace('\n', ' ').replace('\r', '')
        
        # è§£ææ ‡é¢˜ï¼ˆæŒ‰é€—å·æˆ–é¡¿å·åˆ†éš”ï¼‰
        titles = []
        for separator in [',', 'ï¼Œ', 'ã€']:
            if separator in clean_reply:
                titles = [t.strip() for t in clean_reply.split(separator)]
                break
        
        # å¦‚æœæ²¡æœ‰åˆ†éš”ç¬¦ï¼Œç›´æ¥ä½¿ç”¨æ•´ä¸ªå›å¤
        if not titles:
            titles = [clean_reply]
        
        # è¿‡æ»¤å’Œæ¸…ç†æ ‡é¢˜
        valid_titles = []
        for t in titles:
            # ç§»é™¤åºå·ã€æ ‡ç‚¹ã€å¼•å·ç­‰
            t = t.replace('"', '').replace("'", '').replace("ã€‚", '').replace('*', '')
            t = t.replace('**', '').replace('ã€Š', '').replace('ã€‹', '')
            # ç§»é™¤æ•°å­—åºå·
            import re
            t = re.sub(r'^\d+[\.\ã€]?\s*', '', t)
            t = t.strip()
            
            # åªä¿ç•™5ä¸ªå­—ä»¥å†…çš„æ ‡é¢˜
            if t and len(t) <= 5:
                valid_titles.append(t)
        
        if valid_titles:
            # éšæœºé€‰æ‹©ä¸€ä¸ªæ ‡é¢˜
            import random
            selected_title = random.choice(valid_titles)
            print(f"[OK] å¯¹è¯æ ‡é¢˜: {selected_title}")
            return selected_title
        else:
            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆæ ‡é¢˜ï¼Œä»AIå›å¤ä¸­æå–å…³é”®è¯
            words = ai_response[:20].split()
            fallback = words[0] if words else "æ–°å¯¹è¯"
            print(f"[OK] å¯¹è¯æ ‡é¢˜: {fallback}")
            return fallback[:5]  # æœ€å¤š5ä¸ªå­—
            
    except Exception as e:
        print(f"[ERROR] è·å–å¯¹è¯ä¸»é¢˜å¤±è´¥: {e}")
        return "æ–°å¯¹è¯"