#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¢å¤æ‰€æœ‰å¯¹è¯å…ƒæ•°æ®
ä»ç£ç›˜ä¸Šçš„å¯¹è¯æ–‡ä»¶æ¢å¤å…ƒæ•°æ®ä¿¡æ¯
"""

import os
import json
import re
from datetime import datetime

def recover_all_conversations():
    """ä»ç£ç›˜æ–‡ä»¶æ¢å¤æ‰€æœ‰å¯¹è¯å…ƒæ•°æ®"""
    
    data_dir = "C:\\AgentData"
    metadata_file = os.path.join(data_dir, "metadata.json")
    
    print("\n" + "="*70)
    print("å¯¹è¯å…ƒæ•°æ®æ¢å¤å·¥å…·")
    print("="*70 + "\n")
    
    # 1. æ‰«ææ‰€æœ‰å¯¹è¯æ–‡ä»¶
    print("ğŸ“‚ æ­£åœ¨æ‰«æå¯¹è¯æ–‡ä»¶...")
    conversations = {}
    max_id = 0
    
    for filename in os.listdir(data_dir):
        if not filename.startswith("conv_") or not filename.endswith(".txt"):
            continue
        
        # æå–å¯¹è¯IDå’Œæ ‡é¢˜
        # æ ¼å¼: conv_[ID]_[TITLE].txt
        match = re.match(r"conv_(\d+)_(.+)\.txt$", filename)
        if not match:
            print(f"âš ï¸  æ–‡ä»¶æ ¼å¼ä¸åŒ¹é…: {filename}")
            continue
        
        conv_id = match.group(1)
        title = match.group(2)
        file_path = os.path.join(data_dir, filename)
        
        # è·å–æ–‡ä»¶ä¿®æ”¹æ—¶é—´
        mtime = os.path.getmtime(file_path)
        updated_at = datetime.fromtimestamp(mtime).strftime("%Y-%m-%d %H:%M:%S")
        
        # ç»Ÿè®¡æ¶ˆæ¯æ•°
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = [line.strip() for line in f if line.strip()]
                message_count = len(lines)
        except:
            message_count = 0
        
        conversations[conv_id] = {
            "title": title,
            "created_at": updated_at,
            "updated_at": updated_at,
            "message_count": message_count
        }
        
        max_id = max(max_id, int(conv_id))
        
        print(f"âœ… {filename}")
        print(f"   ID={conv_id}, æ ‡é¢˜={title}, æ¶ˆæ¯æ•°={message_count}")
    
    if not conversations:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å¯¹è¯æ–‡ä»¶")
        return
    
    # 2. åˆ›å»ºæ–°çš„å…ƒæ•°æ®
    print(f"\nğŸ’¾ ç”Ÿæˆæ–°çš„å…ƒæ•°æ®æ–‡ä»¶...")
    metadata = {
        "conversations": conversations,
        "next_id": max_id + 1
    }
    
    # 3. å¤‡ä»½æ—§çš„å…ƒæ•°æ®
    if os.path.exists(metadata_file):
        backup_file = metadata_file + ".backup"
        try:
            with open(metadata_file, 'r', encoding='utf-8') as f:
                old_metadata = json.load(f)
            with open(backup_file, 'w', encoding='utf-8') as f:
                json.dump(old_metadata, f, ensure_ascii=False, indent=2)
            print(f"âœ… æ—§å…ƒæ•°æ®å·²å¤‡ä»½: {backup_file}")
        except:
            pass
    
    # 4. å†™å…¥æ–°çš„å…ƒæ•°æ®
    try:
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        print(f"âœ… æ–°å…ƒæ•°æ®å·²å†™å…¥: {metadata_file}")
    except Exception as e:
        print(f"âŒ å†™å…¥å…ƒæ•°æ®å¤±è´¥: {e}")
        return
    
    # 5. éªŒè¯æ¢å¤ç»“æœ
    print(f"\n" + "="*70)
    print("æ¢å¤ç»“æœéªŒè¯")
    print("="*70 + "\n")
    
    with open(metadata_file, 'r', encoding='utf-8') as f:
        recovered_metadata = json.load(f)
    
    total_conversations = len(recovered_metadata["conversations"])
    total_messages = sum(conv.get("message_count", 0) for conv in recovered_metadata["conversations"].values())
    
    print(f"âœ… æ¢å¤æˆåŠŸï¼")
    print(f"   æ€»å¯¹è¯æ•°: {total_conversations}")
    print(f"   æ€»æ¶ˆæ¯æ•°: {total_messages}")
    print(f"   ä¸‹ä¸€ä¸ªå¯¹è¯ID: {recovered_metadata['next_id']}")
    
    print(f"\nå¯¹è¯åˆ—è¡¨:")
    for conv_id in sorted(recovered_metadata["conversations"].keys(), key=lambda x: int(x)):
        conv = recovered_metadata["conversations"][conv_id]
        print(f"   [{conv_id}] {conv['title']} ({conv.get('message_count', 0)} æ¡æ¶ˆæ¯)")
    
    print(f"\n" + "="*70)
    print("âœ… æ‰€æœ‰å¯¹è¯å·²æ¢å¤ï¼ç°åœ¨å¯åŠ¨åº”ç”¨æ—¶åº”è¯¥èƒ½çœ‹åˆ°æ‰€æœ‰å¯¹è¯")
    print("="*70 + "\n")

if __name__ == "__main__":
    recover_all_conversations()
